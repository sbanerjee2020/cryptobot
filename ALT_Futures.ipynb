{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALT_Futures.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbanerjee2020/cryptobot/blob/master/ALT_Futures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PQlidRiyQHb4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predicting ALTCoin Prices\n",
        "\n",
        "Predicting the price of cryptos by harnessing deep learning, machine learning and artificial intelligence"
      ]
    },
    {
      "metadata": {
        "id": "LQlcNuTCQnB3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "### Original source\n",
        "1.  David Sheehan Post: [HERE](https://dashee87.github.io/deep%20learning/python/predicting-cryptocurrency-prices-with-deep-learning/)\n",
        "\n",
        "2.  Notebook:    [HERE](# https://github.com/dashee87/blogScripts/blob/master/Jupyter/2017-11-20-predicting-cryptocurrency-prices-with-deep-learning.ipynb)\n",
        "\n",
        "### Related work\n",
        "3.  Jakob Aungiers:  [HERE](http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price)\n",
        "\n",
        "4. Crypto Currency Trader :  [HERE](https://ethereumprice.org/ground-breaking-metropolis-release-flying-radar/)\n",
        "\n",
        "5. What is Metropolis:  [HERE](https://ethereumprice.org/ground-breaking-metropolis-release-flying-radar/)\n",
        "\n",
        "### LSTM\n",
        "\n",
        "6. Understanding LSTM: [HERE](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "7. Exploring LSTM:  [HERE](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
        "\n",
        "8. Original LSTM Paper:  [HERE](http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "\n",
        "### Time Series\n",
        "\n",
        "9. How to make a Time Series Stationery:  [HERE](https://dashee87.github.io/data%20science/general/A-Road-Incident-Model-Analysis/)\n",
        "\n",
        "### Random Walks\n",
        "10. Why Might Share prices follow Random Walk?  [HERE](https://www.tcd.ie/Economics/assets/pdf/SER/2007/Samuel_Dupernex.pdf)\n",
        "\n",
        "### Keras\n",
        "11. Keras Tutorial for Neural Network Beginners:  [HERE](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/)"
      ]
    },
    {
      "metadata": {
        "id": "rbwJlFAbQWe8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install  Packages"
      ]
    },
    {
      "metadata": {
        "id": "8VFvC4-8MPQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hRzkI2lbUd8N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install lxml\n",
        "!pip install BeautifulSoup4\n",
        "!pip install --upgrade pandas\n",
        "!pip install pydrive\n",
        "!pip install ipywidgets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PI_wucuPrvKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khiTjRBgUd8S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Method\n",
        "\n",
        "We're going to employ a Long Short Term Memory (LSTM) model; it's a particular type of deep learning model that is well suited to time series data (or any data with temporal/spatial/structural order e.g. movies, sentences, etc.). \n",
        "\n",
        "If you wish to truly understand the underlying theory (what kind of crypto enthusiast are you?), then I'd recommend [this blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) or [this blog](http://blog.echen.me/2017/05/30/exploring-lstms/) or the [original (white)paper](http://www.bioinf.jku.at/publications/older/2604.pdf). "
      ]
    },
    {
      "metadata": {
        "id": "F732YzSNUd8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "\n",
        "\n",
        "There's a [dataset on Kaggle](https://www.kaggle.com/mczielinski/bitcoin-historical-data) that details minute by minute Bitcoin prices (plus some other factors) for the last few years (featured on that [other blog post](http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price)). Over this timescale, noise could overwhelm the signal, so we'll opt for daily prices. \n",
        "\n",
        "The issue here is that we may have not sufficient data (we'll have hundreds of rows rather than thousands or millions). In deep learning, no model can overcome a severe lack of data. I also don't want to rely on static files, as that'll complicate the process of updating the model in the future with new data. Instead, we'll aim to pull data from websites and APIs.\n",
        "\n",
        "As we'll be combining multiple cryptos in one model, it's probably a good idea to pull the data from one source. We'll use [coinmarketcap.com](https://coinmarketcap.com). For now, we'll only consider Bitcoin and Ether, but it wouldn't be hard to add the [IOTA](https://coinmarketcap.com/currencies/iota/) using this approach. Before we import the data, we must load some python packages that will make our lives so much easier."
      ]
    },
    {
      "metadata": {
        "id": "k8xyy3a0Ud8U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "# get market info for bitcoin from the start of 2016 to the current day\n",
        "bitcoin_market_info = pd.read_html(\"https://coinmarketcap.com/currencies/bitcoin/historical-data/?start=20130428&end=\"+time.strftime(\"%Y%m%d\"))[0]\n",
        "# convert the date string to the correct date format\n",
        "bitcoin_market_info = bitcoin_market_info.assign(Date=pd.to_datetime(bitcoin_market_info['Date']))\n",
        "# when Volume is equal to '-' convert it to 0\n",
        "bitcoin_market_info.loc[bitcoin_market_info['Volume']==\"-\",'Volume']=0\n",
        "# convert to int\n",
        "bitcoin_market_info['Volume'] = bitcoin_market_info['Volume'].astype('int64')\n",
        "# look at the first few rows\n",
        "bitcoin_market_info.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zasYnQn2Ud8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dArIFddlUd8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get market info for ethereum from the start of 2016 to the current day\n",
        "eth_market_info = pd.read_html(\"https://coinmarketcap.com/currencies/ethereum/historical-data/?start=20130428&end=\"+time.strftime(\"%Y%m%d\"))[0]\n",
        "# convert the date string to the correct date format\n",
        "eth_market_info = eth_market_info.assign(Date=pd.to_datetime(eth_market_info['Date']))\n",
        "# look at the first few rows\n",
        "eth_market_info.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWae5CuSUd8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We loaded some python packages and then imported the table that you see on [this site](https://coinmarketcap.com/currencies/bitcoin/historical-data/). With a little bit of data cleaning, we arrive at the above table. We also do the same thing for ether by simply replacing 'bitcoin' with 'ethereum' in the url.\n",
        "\n",
        "Next, we plot the price and volume of both cryptos over time."
      ]
    },
    {
      "metadata": {
        "id": "WOyI5KBaV3ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Getting Python Version\n",
        "import sys\n",
        "from PIL import Image\n",
        "import io\n",
        "sys.version_info[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kq_dCmkvUd8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Getting the Bitcoin and Eth logos\n",
        "\n",
        "if sys.version_info[0] < 3:\n",
        "    import urllib2 as urllib\n",
        "    bt_img = urllib.urlopen(\"http://logok.org/wp-content/uploads/2016/10/Bitcoin-Logo-640x480.png\")\n",
        "    eth_img = urllib.urlopen(\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Ethereum_logo_2014.svg/256px-Ethereum_logo_2014.svg.png\")\n",
        "else:\n",
        "    import urllib\n",
        "    bt_img = urllib.request.urlopen(\"http://logok.org/wp-content/uploads/2016/10/Bitcoin-Logo-640x480.png\")\n",
        "    eth_img = urllib.request.urlopen(\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Ethereum_logo_2014.svg/256px-Ethereum_logo_2014.svg.png\")\n",
        "\n",
        "image_file = io.BytesIO(bt_img.read())\n",
        "bitcoin_im = Image.open(image_file)\n",
        "\n",
        "image_file = io.BytesIO(eth_img.read())\n",
        "eth_im = Image.open(image_file)\n",
        "width_eth_im , height_eth_im  = eth_im.size\n",
        "eth_im = eth_im.resize((int(eth_im.size[0]*0.4), int(eth_im.size[1]*0.4)), Image.ANTIALIAS)\n",
        "bitcoin_im = bitcoin_im.resize((int(bitcoin_im.size[0]*0.4), int(bitcoin_im.size[1]*0.4)), Image.ANTIALIAS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sDW0Y7bXUd8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7853RDnPUd8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eth_im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSfVkDgGe2ZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "djcnSsXoUd8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.columns =[bitcoin_market_info.columns[0]]+['bt_'+i for i in bitcoin_market_info.columns[1:]]\n",
        "eth_market_info.columns =[eth_market_info.columns[0]]+['eth_'+i for i in eth_market_info.columns[1:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rwvopnsIfb1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SwlcrC-DIafX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Visualize"
      ]
    },
    {
      "metadata": {
        "id": "25xwobovcvp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2,1, gridspec_kw = {'height_ratios':[3, 1]})\n",
        "ax1.set_ylabel('Closing Price ($)',fontsize=12)\n",
        "ax2.set_ylabel('Volume ($ bn)',fontsize=12)\n",
        "ax2.set_yticks([int('%d000000000'%i) for i in range(10)])\n",
        "ax2.set_yticklabels(range(10))\n",
        "ax1.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,7]])\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,7]])\n",
        "ax2.set_xticklabels([datetime.date(i,j,1).strftime('%b %Y')  for i in range(2013,2019) for j in [1,7]])\n",
        "\n",
        "\n",
        "ax1.plot(bitcoin_market_info['Date'].values,bitcoin_market_info['bt_Open*'].values)\n",
        "ax2.bar(bitcoin_market_info['Date'].values,bitcoin_market_info['bt_Volume'].values)\n",
        "fig.tight_layout()\n",
        "fig.figimage(bitcoin_im, 100, 220, zorder=3,alpha=.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUa8PQV5X4-G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Merge  BTC and ETH\n",
        "Add New column:  day_diff  =   (Close  - Open)  /  Open"
      ]
    },
    {
      "metadata": {
        "id": "P4-tH-WXjnua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUXrSC_jlf6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.rename(columns={'bt_Open*':'bt_Open'}, \n",
        "                 inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3JgFzdd_lgIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.rename(columns={'bt_Close**':'bt_Close'}, \n",
        "                 inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AEm0Tk7PlrNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bitcoin_market_info.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4klmgBgUjoE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eth_market_info.rename(columns={'eth_Open*':'eth_Open'}, \n",
        "                 inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLQB-bsLlYvG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eth_market_info.rename(columns={'eth_Close**':'eth_Close'}, \n",
        "                 inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-lfz0IxslUpy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eth_market_info.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p6Npt-G8Ud83",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "market_info = pd.merge(bitcoin_market_info,eth_market_info, on=['Date'])\n",
        "market_info = market_info[market_info['Date']>='2016-01-01']\n",
        "for coins in ['bt_', 'eth_']: \n",
        "    kwargs = { coins+'day_diff': lambda x: (x[coins+'Close']-x[coins+'Open'])/x[coins+'Open']}\n",
        "    print(kwargs)\n",
        "    market_info = market_info.assign(**kwargs)\n",
        "market_info.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nhCNX2rBUd85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training and Test Sets\n",
        "\n",
        "\n",
        "We have some data, so now we need to build a model. In deep learning, the data is typically split into training and test sets. The model is built on the training set and subsequently evaluated on the unseen test set. In time series models, we generally train on one period of time and then test on another separate period. Rather arbitrarily, I'll set the cut-off date to June 1st 2017 (i.e. model will be trained on data before that date and assessed on data after it)."
      ]
    },
    {
      "metadata": {
        "id": "CGXPM4C-Ud86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "split_date = '2018-06-01'\n",
        "fig, (ax1, ax2) = plt.subplots(2,1)\n",
        "ax1.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,7]])\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,7]])\n",
        "ax2.set_xticklabels([datetime.date(i,j,1).strftime('%b %Y')  for i in range(2013,2019) for j in [1,7]])\n",
        "ax1.plot(market_info[market_info['Date'] < split_date]['Date'],\n",
        "         market_info[market_info['Date'] < split_date]['bt_Close'], \n",
        "         color='#B08FC7', label='Training')\n",
        "ax1.plot(market_info[market_info['Date'] >= split_date]['Date'],\n",
        "         market_info[market_info['Date'] >= split_date]['bt_Close'], \n",
        "         color='#8FBAC8', label='Test')\n",
        "ax2.plot(market_info[market_info['Date'] < split_date]['Date'],\n",
        "         market_info[market_info['Date'] < split_date]['eth_Close'], \n",
        "         color='#B08FC7')\n",
        "ax2.plot(market_info[market_info['Date'] >= split_date]['Date'],\n",
        "         market_info[market_info['Date'] >= split_date]['eth_Close'], color='#8FBAC8')\n",
        "ax1.set_xticklabels('')\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax2.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "plt.tight_layout()\n",
        "ax1.legend(bbox_to_anchor=(0.03, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "fig.figimage(bitcoin_im.resize((int(bitcoin_im.size[0]*0.65), int(bitcoin_im.size[1]*0.65)), Image.ANTIALIAS), \n",
        "             200, 260, zorder=3,alpha=.5)\n",
        "fig.figimage(eth_im.resize((int(eth_im.size[0]*0.65), int(eth_im.size[1]*0.65)), Image.ANTIALIAS), \n",
        "             255, 40, zorder=3,alpha=.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-N8ApKPiUd89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lag model\n",
        "\n",
        "You can see that the training period mostly consists of periods when cryptos were relatively cheaper. As such, the training data may not be representative of the test data, undermining the model's ability to generalise to unseen data (you could try to make your data stationary- discussed [here](https://dashee87.github.io/data%20science/general/A-Road-Incident-Model-Analysis/)). But why let negative realities get in the way of baseless optimism? Before we take our deep artificially intelligent machine learning model to the moon, it's worth discussing a simpler model. The most basic model is to set tomorrow's price equal to today's price (which we'll crudely call a lag model). This is how we'd define such a model in mathematical terms:\n",
        "\n",
        "\\begin{align}\n",
        "PredPrice_{t} & = ActualPrice_{t-1}\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "Hb8nzLIMUd8-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trivial lag model: P_t = P_(t-1)\n",
        "fig, (ax1, ax2) = plt.subplots(2,1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax2.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         market_info[market_info['Date']>= split_date]['bt_Close'].values, label='Actual')\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "          market_info[market_info['Date']>= datetime.datetime.strptime(split_date, '%Y-%m-%d') - \n",
        "                      datetime.timedelta(days=1)]['bt_Close'][1:].values, label='Predicted')\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "ax1.set_title('Simple Lag Model (Test Set)')\n",
        "ax2.set_ylabel('Etherum Price ($)',fontsize=12)\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         market_info[market_info['Date']>= split_date]['eth_Close'].values, label='Actual')\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "          market_info[market_info['Date']>= datetime.datetime.strptime(split_date, '%Y-%m-%d') - \n",
        "                      datetime.timedelta(days=1)]['eth_Close'][1:].values, label='Predicted')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNY-2yxlUd9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extending this trivial lag model, [stock prices are commonly treated as random walks](https://www.tcd.ie/Economics/assets/pdf/SER/2007/Samuel_Dupernex.pdf), which can be defined in these mathematical terms:\n",
        "\n",
        "\\begin{align}\n",
        "PredPrice_{t} & = ActualPrice_{t-1} * \\epsilon, \\epsilon \\sim N(\\mu, \\sigma)\n",
        "\\end{align}\n",
        "\n",
        "We'll determine &mu; and &sigma; from the training sets and apply the random walk model to the Bitcoin and Ethereum test sets."
      ]
    },
    {
      "metadata": {
        "id": "A95kTHCmUd9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we may want to make sure the daily change in price follows a normal distribution. We'll plot the histogram of values."
      ]
    },
    {
      "metadata": {
        "id": "5ICfywyTiX9o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Distribution of Volatility"
      ]
    },
    {
      "metadata": {
        "id": "MAEFSEV4Ud9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "ax1.hist(market_info[market_info['Date']< split_date]['bt_day_diff'].values, bins=100)\n",
        "ax2.hist(market_info[market_info['Date']< split_date]['eth_day_diff'].values, bins=100)\n",
        "ax1.set_title('Bitcoin Daily Price Changes')\n",
        "ax2.set_title('Ethereum Daily Price Changes')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc5MYobKiu69",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Single Point Random Walk"
      ]
    },
    {
      "metadata": {
        "id": "tDMA3Bo-Ud9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(202)\n",
        "bt_r_walk_mean, bt_r_walk_sd = np.mean(market_info[market_info['Date']< split_date]['bt_day_diff'].values), \\\n",
        "                         np.std(market_info[market_info['Date']< split_date]['bt_day_diff'].values)\n",
        "bt_random_steps = np.random.normal(bt_r_walk_mean, bt_r_walk_sd, \n",
        "                (max(market_info['Date']).to_pydatetime() - datetime.datetime.strptime(split_date, '%Y-%m-%d')).days + 1)\n",
        "eth_r_walk_mean, eth_r_walk_sd = np.mean(market_info[market_info['Date']< split_date]['eth_day_diff'].values), \\\n",
        "                         np.std(market_info[market_info['Date']< split_date]['eth_day_diff'].values)\n",
        "eth_random_steps = np.random.normal(eth_r_walk_mean, eth_r_walk_sd, \n",
        "                (max(market_info['Date']).to_pydatetime() - datetime.datetime.strptime(split_date, '%Y-%m-%d')).days + 1)\n",
        "fig, (ax1, ax2) = plt.subplots(2,1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax2.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "     market_info[market_info['Date']>= split_date]['bt_Close'].values, label='Actual')\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "      market_info[(market_info['Date']+ datetime.timedelta(days=1))>= split_date]['bt_Close'].values[1:] * \n",
        "     (1+bt_random_steps), label='Predicted')\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "     market_info[market_info['Date']>= split_date]['eth_Close'].values, label='Actual')\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "      market_info[(market_info['Date']+ datetime.timedelta(days=1))>= split_date]['eth_Close'].values[1:] * \n",
        "     (1+eth_random_steps), label='Predicted')\n",
        "ax1.set_title('Single Point Random Walk (Test Set)')\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax2.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bJeIxVjyUd9M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Wow! Look at those prediction lines. Apart from a few kinks, it broadly tracks the actual closing price for each coin. It even captures the eth rises (and subsequent falls) in mid-June and late August. As pointed out on that other blog, models that only make predictions one point into the future are often misleadingly accurate, as errors aren't carried over to subsequent predictions. No matter how large the error, it's essentially reset at each time point, as the model is fed the true price. The Bitcoin random walk is particularly deceptive, as the scale of the y-axis is quite wide, making the prediction line appear quite smooth.\n",
        "\n",
        "Single point predictions are unfortunately quite common when evaluating time series models (e.g.[here](https://medium.com/@binsumi/neural-networks-and-bitcoin-d452bfd7757e) and [here](https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f)). A better idea could be to measure its accuracy on multi-point predictions. That way, errors from previous predictions aren't reset but rather are compounded by subsequent predictions. Thus, poor models are penalised more heavily. In mathematical terms:\n",
        "\n",
        "\\begin{align}\n",
        "PredPrice_{t} & = PredPrice_{t-1} * \\epsilon, \\epsilon \\sim N(\\mu, \\sigma)\\ \\&  \\ PredPrice_0 = Price_0\n",
        "\\end{align}\n",
        "\n",
        "Let's get our random walk model to predict the closing prices over the total test set."
      ]
    },
    {
      "metadata": {
        "id": "F6QB_ibJi18I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Full Interval Random Walk"
      ]
    },
    {
      "metadata": {
        "id": "TOsyYzzzUd9N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bt_random_walk = []\n",
        "eth_random_walk = []\n",
        "for n_step, (bt_step, eth_step) in enumerate(zip(bt_random_steps, eth_random_steps)):\n",
        "    if n_step==0:\n",
        "        bt_random_walk.append(market_info[market_info['Date']< split_date]['bt_Close'].values[0] * (bt_step+1))\n",
        "        eth_random_walk.append(market_info[market_info['Date']< split_date]['eth_Close'].values[0] * (eth_step+1))\n",
        "    else:\n",
        "        bt_random_walk.append(bt_random_walk[n_step-1] * (bt_step+1))\n",
        "        eth_random_walk.append(eth_random_walk[n_step-1] * (eth_step+1))\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax2.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         market_info[market_info['Date']>= split_date]['bt_Close'].values, label='Actual')\n",
        "ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         bt_random_walk[::-1], label='Predicted')\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         market_info[market_info['Date']>= split_date]['eth_Close'].values, label='Actual')\n",
        "ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         eth_random_walk[::-1], label='Predicted')\n",
        "\n",
        "ax1.set_title('Full Interval Random Walk')\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax2.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOta-mZCUd9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model predictions are extremely sensitive to the random seed. I've selected one where the full interval random walk looks almost decent for Ethereum. In the [accompanying Jupyter notebook](https://github.com/dashee87/blogScripts/blob/master/Jupyter/2017-11-20-predicting-cryptocurrency-prices-with-deep-learning.ipynb), you can interactively play around with the seed value below to see how badly it can perform."
      ]
    },
    {
      "metadata": {
        "id": "wZAv9HueUd9R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def plot_func(freq):\n",
        "    np.random.seed(freq)\n",
        "    random_steps = np.random.normal(eth_r_walk_mean, eth_r_walk_sd, \n",
        "                (max(market_info['Date']).to_pydatetime() - datetime.datetime.strptime(split_date, '%Y-%m-%d')).days + 1)\n",
        "    random_walk = []\n",
        "    for n_step,i in enumerate(random_steps):\n",
        "        if n_step==0:\n",
        "            random_walk.append(market_info[market_info['Date']< split_date]['eth_Close'].values[0] * (i+1))\n",
        "        else:\n",
        "            random_walk.append(random_walk[n_step-1] * (i+1))\n",
        "    fig, (ax1, ax2) = plt.subplots(2,1)\n",
        "    ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "    ax1.set_xticklabels('')\n",
        "    ax2.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "    ax2.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "    ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "         market_info[market_info['Date']>= split_date]['eth_Close'].values, label='Actual')\n",
        "    ax1.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "          market_info[(market_info['Date']+ datetime.timedelta(days=1))>= split_date]['eth_Close'].values[1:] * \n",
        "         (1+random_steps), label='Predicted')\n",
        "    ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "          market_info[(market_info['Date']+ datetime.timedelta(days=1))>= split_date]['eth_Close'].values[1:] * \n",
        "         (1+random_steps))\n",
        "    ax2.plot(market_info[market_info['Date']>= split_date]['Date'],\n",
        "             random_walk[::-1])\n",
        "    ax1.set_title('Single Point Random Walk')\n",
        "    ax1.set_ylabel('')\n",
        "    # for static figures, you may wish to insert the random seed value\n",
        "#    ax1.annotate('Random Seed: %d'%freq, xy=(0.75, 0.2),  xycoords='axes fraction',\n",
        "#            xytext=(0.75, 0.2), textcoords='axes fraction')\n",
        "    ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "    ax2.set_title('Full Interval Random Walk')\n",
        "    fig.text(0.0, 0.5, 'Ethereum Price ($)', va='center', rotation='vertical',fontsize=12)\n",
        "    plt.tight_layout()\n",
        "#    plt.savefig('image%d.png'%freq, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "interact(plot_func, freq =widgets.IntSlider(min=200,max=210,step=1,value=205, description='Random Seed:'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAYbn__4Ud9V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how the single point random walk always looks quite accurate, even though there's no real substance behind it. Hopefully, you'll be more suspicious of any blog that claims to accurately predict prices. I probably shouldn't worry; it's not like crypto fans to be seduced by [slick marketing claims](https://uetoken.com/)."
      ]
    },
    {
      "metadata": {
        "id": "Sr5fgYU0Ud9V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory (LSTM)\n",
        "### Keras\n",
        "\n",
        "We will use Keras, as it is most intuitive for non-experts. "
      ]
    },
    {
      "metadata": {
        "id": "auxdsnPfUd9W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for coins in ['bt_', 'eth_']: \n",
        "    kwargs = { coins+'close_off_high': lambda x: 2*(x[coins+'High']- x[coins+'Close'])/(x[coins+'High']-x[coins+'Low'])-1,\n",
        "            coins+'volatility': lambda x: (x[coins+'High']- x[coins+'Low'])/(x[coins+'Open'])}\n",
        "    market_info = market_info.assign(**kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ZWHtSpUUd9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_data = market_info[['Date']+[coin+metric for coin in ['bt_', 'eth_'] \n",
        "                                   for metric in ['Close','Volume','close_off_high','volatility']]]\n",
        "# need to reverse the data frame so that subsequent rows represent later timepoints\n",
        "model_data = model_data.sort_values(by='Date')\n",
        "model_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UQAFAt_jUd9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I've created a new data frame called `model_data`. I've removed some of the previous columns (open price, daily highs and lows) and reformulated some new ones. `close_off_high` represents the gap between the closing price and price high for that day, where values of -1 and 1 mean the closing price was equal to the daily low or daily high, respectively. The `volatility` columns are simply the difference between high and low price divided by the opening price. You may also notice that `model_data` is arranged in order of earliest to latest. We don't actually need the date column anymore, as that information won't be fed into the model.\n",
        "\n",
        "Our LSTM model will use previous data (both bitcoin and eth) to predict the next day's closing price of a specific coin. We must decide how many previous days it will have access to. Again, it's rather arbitrary, but I'll opt for 10 days, as it's a nice round number. We build little data frames consisting of 10 consecutive days of data (called windows), so the first window will consist of the 0-9th rows of the training set (Python is zero-indexed), the second will be the rows 1-10, etc.  Picking a small window size means we can feed more windows into our model; the downside is that the model may not have sufficient information to detect complex long term behaviours (if such things exist). \n",
        "\n",
        "Deep learning models don't like inputs that vary wildly. Looking at those columns, some values range between -1 and 1, while others are on the scale of millions. We need to normalise the data, so that our inputs are somewhat consistent. Typically, you want values between -1 and 1. The `off_high` and `volatility` columns are fine as they are. For the remaining columns, like that [other blog post](http://www.jakob-aungiers.com/articles/a/Multidimensional-LSTM-Networks-to-Predict-Bitcoin-Price), we'll normalise the inputs to the first value in the window."
      ]
    },
    {
      "metadata": {
        "id": "TctdA6XbUd9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we don't need the date columns anymore\n",
        "training_set, test_set = model_data[model_data['Date']<split_date], model_data[model_data['Date']>=split_date]\n",
        "training_set = training_set.drop('Date', 1)\n",
        "test_set = test_set.drop('Date', 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sharMJgqUd9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "window_len = 10\n",
        "norm_cols = [coin+metric for coin in ['bt_', 'eth_'] for metric in ['Close','Volume']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3YqEypKUd9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_training_inputs = []\n",
        "for i in range(len(training_set)-window_len):\n",
        "    temp_set = training_set[i:(i+window_len)].copy()\n",
        "    for col in norm_cols:\n",
        "        temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
        "    LSTM_training_inputs.append(temp_set)\n",
        "LSTM_training_outputs = (training_set['eth_Close'][window_len:].values/training_set['eth_Close'][:-window_len].values)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6IuefMNUd9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_test_inputs = []\n",
        "for i in range(len(test_set)-window_len):\n",
        "    temp_set = test_set[i:(i+window_len)].copy()\n",
        "    for col in norm_cols:\n",
        "        temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
        "    LSTM_test_inputs.append(temp_set)\n",
        "LSTM_test_outputs = (test_set['eth_Close'][window_len:].values/test_set['eth_Close'][:-window_len].values)-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mvxLSKCUUd9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LSTM_training_inputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "td1Qq290Ud9i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This table represents an example of our LSTM model input (we'll actually have hundreds of similar tables). We've normalised some columns so that their values are equal to 0 in the first time point, so we're aiming to predict changes in price relative to this timepoint. We're now ready to build the LSTM model. This is actually quite straightforward with Keras, you simply stack componenets on top of each other (better explained [here](https://dashee87.github.io/data%20science/deep%20learning/python/another-keras-tutorial-for-neural-network-beginners/))."
      ]
    },
    {
      "metadata": {
        "id": "rcRFmWBHUd9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# I find it easier to work with numpy arrays rather than pandas dataframes\n",
        "# especially as we now only have numerical data\n",
        "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
        "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
        "\n",
        "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
        "LSTM_test_inputs = np.array(LSTM_test_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKuXJqMwUd9k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import the relevant Keras modules\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "\n",
        "def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n",
        "                dropout=0.25, loss=\"mae\", optimizer=\"adam\"):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(LSTM(neurons, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(units=output_size))\n",
        "    model.add(Activation(activ_func))\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggeMxmKOUd9n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, the `build_model` functions constructs an empty model unimaginatively called model (`model = Sequential`), to which an LSTM layer is added. That layer has been shaped to fit our inputs (n x m tables, where n and m represent the number of timepoints/rows and columns, respectively). The function also includes more generic neural network features, like [dropout](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/) and [activation functions](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/). Now, we just need to specify the number of neurons to place in the LSTM layer (I've opted for 20 to keep runtime reasonable), as well as the data on which the model will be trained."
      ]
    },
    {
      "metadata": {
        "id": "liLUE9kVUd9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# random seed for reproducibility\n",
        "np.random.seed(202)\n",
        "# initialise model architecture\n",
        "eth_model = build_model(LSTM_training_inputs, output_size=1, neurons = 20)\n",
        "# model output is next price normalised to 10th previous closing price\n",
        "LSTM_training_outputs = (training_set['eth_Close'][window_len:].values/training_set['eth_Close'][:-window_len].values)-1\n",
        "# train model on data\n",
        "# note: eth_history contains information on the training error per epoch\n",
        "eth_history = eth_model.fit(LSTM_training_inputs, LSTM_training_outputs, \n",
        "                            epochs=50, batch_size=1, verbose=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-PJF1SYUd9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Error\n",
        "If everything went to plan, then we'd expect the training error to have gradually decreased over time."
      ]
    },
    {
      "metadata": {
        "id": "SLuF6GiEUd9q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1,1)\n",
        "\n",
        "ax1.plot(eth_history.epoch, eth_history.history['loss'])\n",
        "ax1.set_title('Training Error')\n",
        "\n",
        "if eth_model.loss == 'mae':\n",
        "    ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
        "# just in case you decided to change the model loss calculation\n",
        "else:\n",
        "    ax1.set_ylabel('Model Loss',fontsize=12)\n",
        "ax1.set_xlabel('# Epochs',fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jd0gYQ2bUd9s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate \n",
        "We've just built an LSTM model to predict tomorrow's Ethereum closing price. Let's see how well it performs. We start by examining its performance on the training set (data before June 2017). That number below the code represents the model's mean absolute error (mae) on the training set after the 50th training iteration (or epoch). Instead of relative changes, we can view the model output as daily closing prices."
      ]
    },
    {
      "metadata": {
        "id": "M2DspjoZUd9t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
        "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
        "\n",
        "fig, ax1 = plt.subplots(1,1)\n",
        "ax1.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,5,9]])\n",
        "ax1.set_xticklabels([datetime.date(i,j,1).strftime('%b %Y')  for i in range(2013,2019) for j in [1,5,9]])\n",
        "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         training_set['eth_Close'][window_len:], label='Actual')\n",
        "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         ((np.transpose(eth_model.predict(LSTM_training_inputs))+1) * training_set['eth_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "ax1.set_title('Training Set: Single Timepoint Prediction')\n",
        "ax1.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.15, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "ax1.annotate('MAE: %.4f'%np.mean(np.abs((np.transpose(eth_model.predict(LSTM_training_inputs))+1)-\\\n",
        "            (training_set['eth_Close'].values[window_len:])/(training_set['eth_Close'].values[:-window_len]))), \n",
        "             xy=(0.75, 0.9),  xycoords='axes fraction',\n",
        "            xytext=(0.75, 0.9), textcoords='axes fraction')\n",
        "# figure inset code taken from http://akuederle.com/matplotlib-zoomed-up-inset\n",
        "axins = zoomed_inset_axes(ax1, 3.35, loc=10) # zoom-factor: 3.35, location: centre\n",
        "axins.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,5,9]])\n",
        "axins.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         training_set['eth_Close'][window_len:], label='Actual')\n",
        "axins.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         ((np.transpose(eth_model.predict(LSTM_training_inputs))+1) * training_set['eth_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "axins.set_xlim([datetime.date(2017, 3, 1), datetime.date(2017, 5, 1)])\n",
        "axins.set_ylim([10,60])\n",
        "axins.set_xticklabels('')\n",
        "mark_inset(ax1, axins, loc1=1, loc2=3, fc=\"none\", ec=\"0.5\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ibvLPHfDUd9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We shouldn't be too surprised by its apparent accuracy here. The model could access the source of its error and adjust itself accordingly. In fact, it's not hard to attain almost zero training errors. We could just cram in hundreds of neurons and train for thousands of epochs (a process known as overfitting, where you're essentially predicting noise- I included the `Dropout()` call in the `build_model` function to mitigate this risk for our relatively small model). We should be more interested in its performance on the test dataset, as this represents completely new data for the model."
      ]
    },
    {
      "metadata": {
        "id": "nP3-GjIbUd9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1,1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax1.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:],\n",
        "         test_set['eth_Close'][window_len:], label='Actual')\n",
        "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:],\n",
        "         ((np.transpose(eth_model.predict(LSTM_test_inputs))+1) * test_set['eth_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "ax1.annotate('MAE: %.4f'%np.mean(np.abs((np.transpose(eth_model.predict(LSTM_test_inputs))+1)-\\\n",
        "            (test_set['eth_Close'].values[window_len:])/(test_set['eth_Close'].values[:-window_len]))), \n",
        "             xy=(0.75, 0.9),  xycoords='axes fraction',\n",
        "            xytext=(0.75, 0.9), textcoords='axes fraction')\n",
        "ax1.set_title('Test Set: Single Timepoint Prediction',fontsize=13)\n",
        "ax1.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_EoO371Ud9y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Caveats aside about the misleading nature of single point predictions, our LSTM model *seems* to have performed well on the unseen test set. The most obvious flaw is that it fails to detect the inevitable downturn when the eth price suddenly shoots up (e.g mid-June and October). In fact, this is a persistent failure; it's just more apparent at these spikes. The predicted price regularly seems equivalent to the actual price just shifted one day later (e.g. the drop in mid-July). Furthermore, the model seems to be systemically overestimating the future value of Ether (join the club, right?), as the predicted line near always runs higher than the actual line. I suspect this is because the training data represents a period during which the price of Ether rose astronomically, so it expects that trend to continue (don't we all). We can also build a similar LSTM model for Bitcoin- test set predictions are plotted below (see [Jupyter notebook for full code](https://github.com/dashee87/blogScripts/blob/master/Jupyter/2017-11-20-predicting-cryptocurrency-prices-with-deep-learning.ipynb))."
      ]
    },
    {
      "metadata": {
        "id": "J-SnoOMEUd9y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# random seed for reproducibility\n",
        "np.random.seed(202)\n",
        "# initialise model architecture\n",
        "bt_model = build_model(LSTM_training_inputs, output_size=1, neurons = 20)\n",
        "# train model on data\n",
        "# note: eth_history contains information on the training error per epoch\n",
        "bt_history = bt_model.fit(LSTM_training_inputs, \n",
        "                            (training_set['bt_Close'][window_len:].values/training_set['bt_Close'][:-window_len].values)-1, \n",
        "                            epochs=50, batch_size=1, verbose=2, shuffle=True)\n",
        "# #eth_model.save('eth_model%d.h5'%j)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YKjq_qZ9Ud90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
        "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
        "\n",
        "fig, ax1 = plt.subplots(1,1)\n",
        "ax1.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,5,9]])\n",
        "ax1.set_xticklabels([datetime.date(i,j,1).strftime('%b %Y')  for i in range(2013,2019) for j in [1,5,9]])\n",
        "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         training_set['bt_Close'][window_len:], label='Actual')\n",
        "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         ((np.transpose(bt_model.predict(LSTM_training_inputs))+1) * training_set['bt_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "ax1.set_title('Training Set: Single Timepoint Prediction')\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax1.annotate('MAE: %.4f'%np.mean(np.abs((np.transpose(bt_model.predict(LSTM_training_inputs))+1)-\\\n",
        "            (training_set['bt_Close'].values[window_len:])/(training_set['bt_Close'].values[:-window_len]))), \n",
        "             xy=(0.75, 0.9),  xycoords='axes fraction',\n",
        "            xytext=(0.75, 0.9), textcoords='axes fraction')\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "# figure inset code taken from http://akuederle.com/matplotlib-zoomed-up-inset\n",
        "axins = zoomed_inset_axes(ax1, 2.52, loc=10, bbox_to_anchor=(400, 307)) # zoom-factor: 2.52, location: centre\n",
        "axins.set_xticks([datetime.date(i,j,1) for i in range(2013,2019) for j in [1,5,9]])\n",
        "axins.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         training_set['bt_Close'][window_len:], label='Actual')\n",
        "axins.plot(model_data[model_data['Date']< split_date]['Date'][window_len:],\n",
        "         ((np.transpose(bt_model.predict(LSTM_training_inputs))+1) * training_set['bt_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "axins.set_xlim([datetime.date(2017, 2, 15), datetime.date(2017, 5, 1)])\n",
        "axins.set_ylim([920, 1400])\n",
        "axins.set_xticklabels('')\n",
        "mark_inset(ax1, axins, loc1=1, loc2=3, fc=\"none\", ec=\"0.5\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ahiBId5Ud92",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(1,1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax1.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][10:],\n",
        "         test_set['bt_Close'][window_len:], label='Actual')\n",
        "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][10:],\n",
        "         ((np.transpose(bt_model.predict(LSTM_test_inputs))+1) * test_set['bt_Close'].values[:-window_len])[0], \n",
        "         label='Predicted')\n",
        "ax1.annotate('MAE: %.4f'%np.mean(np.abs((np.transpose(bt_model.predict(LSTM_test_inputs))+1)-\\\n",
        "            (test_set['bt_Close'].values[window_len:])/(test_set['bt_Close'].values[:-window_len]))), \n",
        "             xy=(0.75, 0.9),  xycoords='axes fraction',\n",
        "            xytext=(0.75, 0.9), textcoords='axes fraction')\n",
        "ax1.set_title('Test Set: Single Timepoint Prediction',fontsize=13)\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "34OQAU6mUd95",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Next 5 day Prediction\n",
        "As I've stated earlier, single point predictions can be deceptive. Looking more closely, you'll notice that, again, the predicted values regularly mirror the previous values (e.g. October). Our fancy deep learning LSTM model has partially reproducted a [autregressive (AR) model](https://dashee87.github.io/data%20science/general/A-Road-Incident-Model-Analysis/) of some order `p`, where future values are simply the weighted sum of the previous `p` values. We can define an AR model in these mathematical terms:  \n",
        "\n",
        "\\begin{align}\n",
        "PredPrice_{t} & = \\phi_0 + \\phi_1*Price_{t-1} + \\ldots + \\phi_p*Price_{t-p} + \\epsilon_t, \\ \\epsilon_t \\sim N(0, \\sigma)\\\n",
        "\\end{align}\n",
        "\n",
        "The good news is that AR models are commonly employed in time series tasks (e.g. [stock market prices](https://upcommons.upc.edu/bitstream/handle/2099/3572/04marcek.pdf)), so the LSTM model appears to have landed on a sensible solution. The bad news is that it's a waste of the LSTM capabilities, we could have a built a much simpler AR model in much less time and probably achieved similar results (though the title of this post would have been much less clickbaity). [More complex does not automatically equal more accurate](https://pdfs.semanticscholar.org/696c/2fa5697f58914921ff37d69ced44ddea143f.pdf)). \n",
        "\n",
        "We'll now build LSTM models to predict crypto prices for the next 5 days."
      ]
    },
    {
      "metadata": {
        "id": "8WLvWTatUd95",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# random seed for reproducibility\n",
        "np.random.seed(202)\n",
        "# we'll try to predict the closing price for the next 5 days \n",
        "# change this value if you want to make longer/shorter prediction\n",
        "pred_range = 5\n",
        "# initialise model architecture\n",
        "eth_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 20)\n",
        "# model output is next 5 prices normalised to 10th previous closing price\n",
        "LSTM_training_outputs = []\n",
        "for i in range(window_len, len(training_set['eth_Close'])-pred_range):\n",
        "    LSTM_training_outputs.append((training_set['eth_Close'][i:i+pred_range].values/\n",
        "                                  training_set['eth_Close'].values[i-window_len])-1)\n",
        "LSTM_training_outputs = np.array(LSTM_training_outputs)\n",
        "# train model on data\n",
        "# note: eth_history contains information on the training error per epoch\n",
        "eth_history = eth_model.fit(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, \n",
        "                            epochs=50, batch_size=1, verbose=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q7v_VpBpUd98",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# random seed for reproducibility\n",
        "np.random.seed(202)\n",
        "# we'll try to predict the closing price for the next 5 days \n",
        "# change this value if you want to make longer/shorter prediction\n",
        "pred_range = 5\n",
        "# initialise model architecture\n",
        "bt_model = build_model(LSTM_training_inputs, output_size=pred_range, neurons = 20)\n",
        "# model output is next 5 prices normalised to 10th previous closing price\n",
        "LSTM_training_outputs = []\n",
        "for i in range(window_len, len(training_set['bt_Close'])-pred_range):\n",
        "    LSTM_training_outputs.append((training_set['bt_Close'][i:i+pred_range].values/\n",
        "                                  training_set['bt_Close'].values[i-window_len])-1)\n",
        "LSTM_training_outputs = np.array(LSTM_training_outputs)\n",
        "# train model on data\n",
        "# note: eth_history contains information on the training error per epoch\n",
        "bt_history = bt_model.fit(LSTM_training_inputs[:-pred_range], LSTM_training_outputs, \n",
        "                            epochs=50, batch_size=1, verbose=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmPjI0tJUd9-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# little bit of reformatting the predictions to closing prices\n",
        "eth_pred_prices = ((eth_model.predict(LSTM_test_inputs)[:-pred_range][::pred_range]+1)*\\\n",
        "                   test_set['eth_Close'].values[:-(window_len + pred_range)][::5].reshape(int(np.ceil((len(LSTM_test_inputs)-pred_range)/float(pred_range))),1))\n",
        "bt_pred_prices = ((bt_model.predict(LSTM_test_inputs)[:-pred_range][::pred_range]+1)*\\\n",
        "                   test_set['bt_Close'].values[:-(window_len + pred_range)][::5].reshape(int(np.ceil((len(LSTM_test_inputs)-pred_range)/float(pred_range))),1))\n",
        "\n",
        "pred_colors = [\"#FF69B4\", \"#5D6D7E\", \"#F4D03F\",\"#A569BD\",\"#45B39D\"]\n",
        "fig, (ax1, ax2) = plt.subplots(2,1)\n",
        "ax1.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax2.set_xticks([datetime.date(2017,i+1,1) for i in range(12)])\n",
        "ax2.set_xticklabels([datetime.date(2017,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
        "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:],\n",
        "         test_set['bt_Close'][window_len:], label='Actual')\n",
        "ax2.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:],\n",
        "         test_set['eth_Close'][window_len:], label='Actual')\n",
        "for i, (eth_pred, bt_pred) in enumerate(zip(eth_pred_prices, bt_pred_prices)):\n",
        "    # Only adding lines to the legend once\n",
        "    if i<5:\n",
        "        ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:][i*pred_range:i*pred_range+pred_range],\n",
        "                 bt_pred, color=pred_colors[i%5], label=\"Predicted\")\n",
        "    else: \n",
        "        ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:][i*pred_range:i*pred_range+pred_range],\n",
        "                 bt_pred, color=pred_colors[i%5])\n",
        "    ax2.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:][i*pred_range:i*pred_range+pred_range],\n",
        "             eth_pred, color=pred_colors[i%5])\n",
        "ax1.set_title('Test Set: 5 Timepoint Predictions',fontsize=13)\n",
        "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
        "ax1.set_xticklabels('')\n",
        "ax2.set_ylabel('Ethereum Price ($)',fontsize=12)\n",
        "ax1.legend(bbox_to_anchor=(0.13, 1), loc=2, borderaxespad=0., prop={'size': 12})\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6xuleNiUd-B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  25 RANDOM Seeds\n",
        "\n",
        "The predictions are visibly less impressive than their single point counterparts. Nevertheless, I'm pleased that the model returned somewhat nuanced behaviours (e.g. the second line on the eth graph); it didn't simply forecast prices to move uniformly in one direction. So there are some grounds for optimism.\n",
        "\n",
        "Moving back to the single point predictions, our deep machine artificial neural model looks okay, but so did that boring random walk model. Like the random walk model, LSTM models can be sensitive to the choice of random seed (the model weights are initially randomly assigned). So, if we want to compare the two models, we'll run each one multiple (say, 25) times to get an estimate for the model error. The error will be calculated as the absolute difference between the actual and predicted closing prices changes in the test set."
      ]
    },
    {
      "metadata": {
        "id": "WwrU7NwSUd-B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# warning: this cell may take some time to execute\n",
        "# this code builds 25 LSTM models for eth and bitcoin each (with 25 different initialisations)\n",
        "# the models are then saved so you hopefully only need to run this cell once\n",
        "\n",
        "# random seed for reproducibility\n",
        "for rand_seed in range(775,800):\n",
        "    print(rand_seed)\n",
        "    np.random.seed(rand_seed)\n",
        "    temp_model = build_model(LSTM_training_inputs, output_size=1, neurons = 20)\n",
        "    temp_model.fit(LSTM_training_inputs,\n",
        "                   (training_set['eth_Close'][window_len:].values/training_set['eth_Close'][:-window_len].values)-1,\n",
        "                 epochs=50, batch_size=1, verbose=0, shuffle=True)\n",
        "    temp_model.save('eth_model_randseed_%d.h5'%rand_seed)\n",
        "    \n",
        "    file = drive.CreateFile({'parents':[{u'id': '1m63jENcDTcn_ytUSTZSnjBksbDyhnAwH'}]}) \n",
        "    file.SetContentFile('eth_model_randseed_%d.h5'%rand_seed)\n",
        "    file.Upload()\n",
        "    \n",
        "    temp_model = build_model(LSTM_training_inputs, output_size=1, neurons = 20)\n",
        "    temp_model.fit(LSTM_training_inputs,\n",
        "    (training_set['bt_Close'][window_len:].values/training_set['bt_Close'][:-window_len].values)-1,\n",
        "                   epochs=50, batch_size=1, verbose=0, shuffle=True)\n",
        "    temp_model.save('bt_model_randseed_%d.h5'%rand_seed)\n",
        "    \n",
        "    file = drive.CreateFile({'parents':[{u'id': '1m63jENcDTcn_ytUSTZSnjBksbDyhnAwH'}]}) \n",
        "    file.SetContentFile('bt_model_randseed_%d.h5'%rand_seed)\n",
        "    file.Upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rA6VmO9ZQux-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#file = drive.CreateFile({'parents':[{u'id': '1m63jENcDTcn_ytUSTZSnjBksbDyhnAwH'}]}) \n",
        "#file.SetContentFile('bt_model_randseed_777.h5')\n",
        "#file.Upload()\n",
        "\n",
        "\n",
        "#file_list = drive.ListFile({'q': \"'1m63jENcDTcn_ytUSTZSnjBksbDyhnAwH' in parents and trashed=false\"}).GetList()\n",
        "#for file1 in file_list:\n",
        "#  print('title: %s, id: %s' % (file1['title'], file1['id']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n8bd5jXdAphj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsIyt-T7Qloj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_buffer(file_id, verbose=0):\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  request = drive_service.files().get_media(fileId=file_id)\n",
        "  downloaded = io.BytesIO()\n",
        "  downloader = MediaIoBaseDownload(downloaded, request)\n",
        "  done = False\n",
        "  while done is False:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "    progress, done = downloader.next_chunk()\n",
        "    if verbose:\n",
        "      sys.stdout.flush()\n",
        "      sys.stdout.write('\\r')\n",
        "      percentage_done = progress.resumable_progress * 100/progress.total_size\n",
        "      sys.stdout.write(\"[%-100s] %d%%\" % ('='*int(percentage_done), int(percentage_done)))\n",
        "  downloaded.seek(0)\n",
        "  return downloaded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zBQCKkXaQn4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#downloaded_buff = get_file_buffer ('1urJXer3YFpgVnpOXVVrUw8RrTFlnSSzO')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8il_xKeQwBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#downloaded_buff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ggWLeDZIKD5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#with open('/tmp/bt_model_randseed_777.h5', 'wb') as f:\n",
        "#    f.write(downloaded_buff.read())\n",
        "    \n",
        "#files.download('/tmp/bt_model_randseed_777.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9DmO_SIUNDt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls -al /tmp/bt_model_randseed_777.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HosA1o__j0R-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Models saved locally"
      ]
    },
    {
      "metadata": {
        "id": "9W1JQuyjXaK3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -al \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuZg1NNNjyuI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5nQjm-TvKLaf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from keras.models import load_model\n",
        "#temp_model_file = h5py.File('/tmp/bt_model_randseed_777.h5')\n",
        "#temp_model = load_model('eth_model_randseed_777.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bazLZcPMXjc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#temp_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_to89JBZj8cf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Average MAE Across seeds"
      ]
    },
    {
      "metadata": {
        "id": "RlguWdTVUd-D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# assuming the previous cell was completed, this cell loads in the different initialisations\n",
        "# and calculates the average mean absolute error (mae)\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "eth_preds = []\n",
        "bt_preds = []\n",
        "for rand_seed in range(775,800):\n",
        "    temp_model = load_model('eth_model_randseed_%d.h5'%rand_seed)\n",
        "    eth_preds.append(np.mean(abs(np.transpose(temp_model.predict(LSTM_test_inputs))-\n",
        "                (test_set['eth_Close'].values[window_len:]/test_set['eth_Close'].values[:-window_len]-1))))\n",
        "    temp_model = load_model('bt_model_randseed_%d.h5'%rand_seed)\n",
        "    bt_preds.append(np.mean(abs(np.transpose(temp_model.predict(LSTM_test_inputs))-\n",
        "                (test_set['bt_Close'].values[window_len:]/test_set['bt_Close'].values[:-window_len]-1))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvxznybWUd-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eth_random_walk_preds = []\n",
        "bt_random_walk_preds = []\n",
        "for rand_seed in range(775,800):\n",
        "    np.random.seed(rand_seed)\n",
        "    eth_random_walk_preds.append(\n",
        "        np.mean(np.abs((np.random.normal(eth_r_walk_mean, eth_r_walk_sd, len(test_set)-window_len)+1)-\n",
        "                       np.array(test_set['eth_Close'][window_len:])/np.array(test_set['eth_Close'][:-window_len]))))\n",
        "    bt_random_walk_preds.append(\n",
        "    np.mean(np.abs((np.random.normal(bt_r_walk_mean, bt_r_walk_sd, len(test_set)-window_len)+1)-\n",
        "                       np.array(test_set['bt_Close'][window_len:])/np.array(test_set['bt_Close'][:-window_len]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DummmtupUd-G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "ax1.boxplot([bt_preds, bt_random_walk_preds],widths=0.75)\n",
        "ax1.set_ylim([0, 0.2])\n",
        "ax2.boxplot([eth_preds, eth_random_walk_preds],widths=0.75)\n",
        "ax2.set_ylim([0, 0.2])\n",
        "ax1.set_xticklabels(['LSTM', 'Random Walk'])\n",
        "ax2.set_xticklabels(['LSTM', 'Random Walk'])\n",
        "ax1.set_title('Bitcoin Test Set (25 runs)')\n",
        "ax2.set_title('Ethereum Test Set (25 runs)')\n",
        "ax2.set_yticklabels('')\n",
        "ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnCPdFTYUd-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Maybe AI is worth the hype after all! Those graphs show the error on the test set after 25 different initialisations of each model. The LSTM model returns an average error of about 0.04 and 0.05 on the bitcoin and eth prices, respectively, crushing the corresponding random walk models.\n",
        "\n",
        "Aiming to beat random walks is a pretty low bar. It would be more interesting to compare the LSTM model against more appropriate time series models (weighted average, autoregression, [ARIMA](https://dashee87.github.io/data%20science/general/A-Road-Incident-Model-Analysis/) or Facebook's [Prophet algorithm](https://github.com/facebook/prophet)). On the other hand, I'm sure it wouldn't be hard to improve our LSTM model (gratuitously adding more layers and/or neurons, changing the batch size, learning rate, etc.). That said, hopefully you've detected my scepticism when it comes to applying deep learning to predict changes in crypto prices. That's because we're overlooking the best framework of all: human intelligence. Clearly, the perfect model* for predicting cryptos is:\n",
        "\n",
        "\\begin{align}\n",
        "& Price^{Crypto}_{t} > Price^{Crypto}_{t-1} \\,  \\forall \\ Crypto \\neq OmiseGo \\textrm{, }  \\\\\n",
        "& \\textrm{  where } Price^{Crypto}_{t} \\to moon \\textrm{, as } t \\to    \\infty           \n",
        "\\end{align}\n",
        "\n",
        "&#42; This blog does not constitute financial advice and should not be taken as such. While cryptocurrency investments will definitely go up in value forever, they may also go down.\n",
        "\n",
        "# Summary\n",
        "\n",
        "Collected some crypto data and fed it into a deeply intelligent machine learning LSTM model. Unfortunately, its predictions were not that different from just spitting out the previous value. How can we make the model learn more sophisticated behaviours?\n",
        "\n",
        "* **Change Loss Function**: MAE doesn't really encourage risk taking. For example, under mean squared error (MSE), the LSTM model would be forced to place more importance on detecting spikes/troughs. [More bespoke trading focused loss functions](http://www.faculty.ucr.edu/~taelee/paper/lossfunctions.pdf) could also move the model towards less conservative behaviours.\n",
        "\n",
        "* **Penalise conservative AR-type models**: This would incentivise the deep learning algorithm to explore more risky/interesting models. Easier said than done!\n",
        "\n",
        "* **Get more and/or better data**: If past prices alone are sufficient to decently forecast future prices, we need to include other features that provide comparable predictive power. That way, the LSTM model wouldn't be so reliant on past prices, potentially unlocking more complex behaviours. This is probably the best and hardest solution.\n",
        "\n",
        "It is entirely possible that there is no detectable pattern to changes in crypto prices; that no model (however deep) can separate the signal from the noise (similar to the merits of using [deep learning to predict earthquakes](https://www.scientificamerican.com/article/can-artificial-intelligence-predict-earthquakes/)). And any pattern that does appear [can disappear as quickly](http://site.iugaza.edu.ps/wdaya/files/2013/03/A-Random-Walk-Down-Wall-Street.pdf) (see [efficient market hypothesis](results_merge.click2)). \n",
        "\n",
        "Just think how different Bitcoin in 2016 is to craze-riding Bitcoin of late 2017. Any model built on 2016 data would surely struggle to replicate these unprecedented movements. All of this suggests you might as well save yourself some time and stick to autoregression (unless you're writing a blog, of course).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "BrCyRvfbPNYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}